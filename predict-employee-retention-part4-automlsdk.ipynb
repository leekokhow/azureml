{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #4: Automated ML using Azure Machine Learning SDK\n",
    "In this tutorial, we explore how to code [AutoML](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml) using [Azure Machine Learning SDK for Python](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py).\n",
    "Automated ML picks an algorithm and hyperparameters for you and generates a model ready for deployment.\n",
    "\n",
    "Note: This notebook is tested with Azure ML SDK Version 1.24.0. Make sure you change Kernel to \"Python 3.6 - AzureML\" when running this notebook in Azure.\n",
    "\n",
    "## Configure workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. A [Workspace](https://docs.microsoft.com/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py) is a class that accepts your Azure subscription and resource information. It also creates a cloud resource to monitor and track your model runs. `Workspace.from_config()` reads the file **config.json** and loads the authentication details into an object named `ws`. `ws` is used throughout the rest of the code in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from azureml.core import Dataset\n",
    "\n",
    "# Read dataset.\n",
    "clean_df = pd.read_csv('training-data.csv').rename(columns={\"sales\": \"department\"})\n",
    "\n",
    "# Map salary into integers\n",
    "salary_map = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "clean_df[\"salary\"] = clean_df[\"salary\"].map(salary_map)\n",
    "\n",
    "# Create dummy variables for department feature\n",
    "clean_df = pd.get_dummies(clean_df, columns=[\"department\"], drop_first=True)\n",
    "\n",
    "print(clean_df.info())\n",
    "\n",
    "# Note: the \"left\" column is the label (1=left the company, 0=stay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Automated Machine Learning\n",
    "\n",
    "The automated model training involves the following steps:\n",
    "\n",
    "1. Define settings for the experiment run. Attach your training data to the configuration, and modify settings that control the training process.\n",
    "\n",
    "\n",
    "2. Submit the experiment for model tuning. After submitting the experiment, the process iterates through different machine learning algorithms and hyperparameter settings, adhering to your defined constraints. It chooses the best-fit model by optimizing an accuracy metric.\n",
    "\n",
    "\n",
    "The studio automated ML (web interface) uses a remote compute target for model training. But when you use the Python SDK, you can choose either a local compute or a remote compute target:\n",
    "\n",
    "- Local compute: Training occurs on your local laptop or VM compute.\n",
    "- Remote compute: Training occurs on Machine Learning compute clusters.\n",
    "\n",
    "See [here](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#choose-compute-target) for pros and cons of local vs remote compute.\n",
    "\n",
    "Automated ML settings are classified as\n",
    "- [Experiment settings](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#experiment-settings)\n",
    "- [Model settings](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#model-settings)\n",
    "- [Run control settings](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#run-control-settings)\n",
    "\n",
    "These settings are set by instantiating an [`AutoMLConfig`](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py) object and specify a value for various parameters.\n",
    "\n",
    "|Parameter| Value |Description|\n",
    "|----|----|---|\n",
    "|**iteration_timeout_minutes**|10|Time limit in minutes for each iteration. Increase this value for larger datasets that need more time for each iteration.|\n",
    "|**iterations**|6|The total number of different algorithm and parameter combinations to test during an automated ML experiment. If not specified, the default is 1000 iterations.|                         \n",
    "|**experiment_timeout_hours**|0.3|Maximum amount of time in hours that all iterations combined can take before the experiment terminates.|\n",
    "|**enable_early_stopping**|True|Flag to enable early termination if the score is not improving in the short term.|\n",
    "|**primary_metric**| AUC_weighted | Metric that you want to optimize. The best-fit model will be chosen based on this metric.|\n",
    "|**featurization**| auto | By using auto, the experiment can preprocess the input data (handling missing data, converting text to numeric, etc.)|\n",
    "|**verbosity**| logging.INFO | Controls the level of logging.|\n",
    "\n",
    "See [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train#configure-your-experiment-settings) for examples of configuring `AutoMLConfig`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\": 10,\n",
    "    \"iterations\": 6,\n",
    "    \"experiment_timeout_hours\": 0.3,\n",
    "    \"enable_early_stopping\": True,\n",
    "    \"primary_metric\": 'AUC_weighted',\n",
    "    \"featurization\": 'auto',\n",
    "    \"verbosity\": logging.INFO\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other settings include:\n",
    "#### 1. Experiment type\n",
    "\n",
    "The kind of machine learning problem you are solving is specified in the `task` parameter.\n",
    "\n",
    "The supported `task` types are `classification`, `regression`, and `forecasting`.\n",
    "\n",
    "When to use which `task` is define [here](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#when-to-use-automl-classify-regression--forecast).\n",
    "\n",
    "#### 2. Allowed or blocked models\n",
    "\n",
    "Automated machine learning tries different algorithms during the automation and tuning process.\n",
    "\n",
    "By default, the three different `task` parameter values determine the list of algorithms, or models, to apply.\n",
    "\n",
    "However you can use `allowed_models` or `blocked_models` parameters to include or exclude algorithms.\n",
    "\n",
    "See supported models by task type [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train#supported-models).\n",
    "\n",
    "#### 3. Automatic data featurization\n",
    "\n",
    "The following table shows the accepted settings for featurization in the AutoMLConfig class:\n",
    "\n",
    "|Featurization configuration|Description|\n",
    "|----|----|\n",
    "|\"featurization\": 'auto'|Specifies that, as part of preprocessing, [data guardrails](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-features#data-guardrails) and<br>[featurization steps](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-features#featurization) are to be done automatically.<br>This setting is the default.|\n",
    "|\"featurization\": 'off'|Specifies that [featurization steps](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-features#featurization) are not to be done automatically.|\n",
    "|\"featurization\": 'FeaturizationConfig'|Specifies that [customized featurization](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-features#customize-featurization) steps are to be used.|\n",
    "                                                                                                                                                                     \n",
    "Note: [Automated machine learning featurization](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-features) steps (feature normalization, handling missing data, converting text to numeric, etc.) become part of the underlying model. When using the model for predictions, the same featurization steps applied during training are applied to your input data automatically.\n",
    "                                         \n",
    "#### 4. Primary Metric\n",
    "The primary metric parameter determines the metric to be used during model training for optimization. \n",
    "\n",
    "The available metrics you can select is determined by the task type you choose.\n",
    "                                                                                  \n",
    "See valid primary metrics for each task type [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train#primary-metric).\n",
    "\n",
    "#### 5. K-fold cross-validation\n",
    "\n",
    "To perform k-fold cross-validation, include the `n_cross_validations`.\n",
    "                                                    \n",
    "This parameter sets how many cross validations to perform, based on the same number of folds.\n",
    "\n",
    "In the following code, five folds for cross-validation are defined: `n_cross_validations=5`\n",
    "\n",
    "Hence, five different trainings, each training using 4/5 of the data, and each validation using 1/5 of the data with a different holdout fold each time. As a result, metrics are calculated with the average of the five validation metrics.\n",
    "\n",
    "Note: If you do not explicitly specify either a `validation_data` or `n_cross_validations` parameter, automated ML applies default techniques depending on the number of rows provided in the single dataset `training_data`:\n",
    "\n",
    "|Training data size|Validation technique|\n",
    "|----|----|\n",
    "|Larger than 20,000 rows|Train/validation data split is applied.<br><br>The default is to take 10% of the initial training data set as the<br>validation set. In turn, that validation set is used for metrics calculation.|\n",
    "|Smaller than 20,000 rows|Cross-validation approach is applied.<br><br>The default number of folds depends on the number of rows.<br><br>If the dataset is less than 1,000 rows, 10 folds are used.<br>If the rows are between 1,000 and 20,000, then three folds are used.|\n",
    "\n",
    "Note: The `n_cross_validations` parameter is not supported in classification scenarios that use deep neural networks.\n",
    "\n",
    "#### 6. Ensemble configuration\n",
    "\n",
    "[Ensemble models](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#ensemble) are enabled by default, and appear as the final run iterations in an AutoML run.\n",
    "\n",
    "Currently VotingEnsemble and StackEnsemble are supported.\n",
    "\n",
    "Ensemble training can be disabled by using the `enable_voting_ensemble` and `enable_stack_ensemble` boolean parameters.\n",
    "                  \n",
    "See [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train#ensemble-configuration) for configuration.\n",
    "                                                                                                                               \n",
    "                                                                                                                               \n",
    "#### 7. Exit criteria\n",
    "\n",
    "Define the exit criteria in your AutoMLConfig to end your experiment.\n",
    "\n",
    "|Criteria|description|\n",
    "|----|----|\n",
    "|No criteria|If you do not define any exit parameters the experiment continues<br>until no further progress is made on your primary metric.|\n",
    "|After a length of time|Use `experiment_timeout_minutes` in your settings<br>to define how long, in minutes, your experiment should continue to run.<br><br>To help avoid experiment time out failures, there is a minimum of<br>15 minutes, or 60 minutes if your row by column size exceeds 10 million.|\n",
    "|A score has been reached|Use `experiment_exit_score` completes the experiment<br>after a specified primary metric score has been reached.|\n",
    "\n",
    "                                                                                                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Provide validation data or configure a validation data size\n",
    "\n",
    "You can either start with a single data file and split it into training and validation datasets, or provide a separate data file for the validation set.\n",
    "\n",
    "`validation_data` parameter assigns which data to use as your validation set, it accepts an Azure Machine Learning dataset or pandas dataframe.\n",
    "\n",
    "Note: Use your previously defined training settings as a `**kwargs` parameter to an `AutoMLConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Note the split here does not separate the label column \"left\".\n",
    "x_train, x_test = train_test_split(clean_df, test_size=0.2, random_state=223)\n",
    "\n",
    "automl_config = AutoMLConfig(task='classification',\n",
    "                             training_data=x_train,\n",
    "                             validation_data=x_test, # Cannot specify n_cross_validations when validation_data is specified\n",
    "                             label_column_name=\"left\", # Indicate the label column here\n",
    "                             blocked_models=['RandomForest'],\n",
    "                             enable_voting_ensemble=False,\n",
    "                             enable_stack_ensemble=False,\n",
    "                             debug_log='automl_errors.log',\n",
    "                             **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can set the `validation_size` parameter to hold out a portion of the training data for validation. This means that the validation set will be split by automated ML from the initial `training_data` provided.\n",
    "\n",
    "This value should be between `0.0` and `1.0` non-inclusive (for example, 0.2 means 20% of the data is held out for validation data).\n",
    "\n",
    "Note: The `validation_size` parameter is not supported in forecasting scenarios.\n",
    "\n",
    "Note: To perform [Monte Carlo cross-validation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-cross-validation-data-splits#monte-carlo-cross-validation) must specify both `validation_size` and `n_cross_validations` parameters.\n",
    "\n",
    "    automl_config = AutoMLConfig(task='classification',\n",
    "                                 training_data=clean_df,\n",
    "                                 validation_size=0.2,\n",
    "                                 n_cross_validations=5,\n",
    "                                 label_column_name=\"left\",\n",
    "                                 ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Create an experiment object in your workspace. An experiment acts as a container for your individual runs.\n",
    "\n",
    "Pass the defined `automl_config` object to the experiment, and set the output to `True` to view progress during the run.\n",
    "\n",
    "After starting the experiment, the output shown updates live as the experiment runs.\n",
    "\n",
    "For each iteration, it shows the pipeline summary, run duration, and the primary metric score. The `BEST` field tracks the best metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, \"predict-employee-retention-automl-sdk\")\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the results\n",
    "\n",
    "Explore the results of automatic training with a [Jupyter widget](https://docs.microsoft.com/python/api/azureml-widgets/azureml.widgets?view=azure-ml-py).\n",
    "\n",
    "The widget allows you to see a graph and table of all individual run iterations and metric score after the run completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(local_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Monitor automated machine learning runs\n",
    "\n",
    "To access the charts from a previous automated ML run, replace the `run_id` in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "run_id = 'AutoML_5cb4b0d0-99f2-4951-bf12-15d70fc83843'\n",
    "experiment = Experiment(ws,\"predict-employee-retention-automl-sdk\")\n",
    "run = Run(experiment, run_id)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the best model\n",
    "\n",
    "To select the best model from the iterations, use `get_output` to get the best run and the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def print_model(model, prefix=\"\"):\n",
    "    for step in model.steps:\n",
    "        print(prefix + step[0])\n",
    "        if hasattr(step[1], 'estimators') and hasattr(step[1], 'weights'):\n",
    "            pprint({'estimators': list(e[0] for e in step[1].estimators), 'weights': step[1].weights})\n",
    "            print()\n",
    "            for estimator in step[1].estimators:\n",
    "                print_model(estimator[1], estimator[0]+ ' - ')\n",
    "        elif hasattr(step[1], '_base_learners') and hasattr(step[1], '_meta_learner'):\n",
    "            print(\"\\nMeta Learner\")\n",
    "            pprint(step[1]._meta_learner)\n",
    "            print()\n",
    "            for estimator in step[1]._base_learners:\n",
    "                print_model(estimator[1], estimator[0]+ ' - ')\n",
    "        else:\n",
    "            pprint(step[1].get_params())\n",
    "            print()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "\n",
    "print('BEST RUN:')\n",
    "print(best_run)\n",
    "print('\\nFITTED MODEL:')\n",
    "#print(fitted_model)\n",
    "print_model(fitted_model)\n",
    "print('\\nFITTED MODEL STEPS:')\n",
    "print(fitted_model.steps)\n",
    "print('\\nMETRICS:')\n",
    "metrics = best_run.get_metrics()\n",
    "for metric_name in metrics:\n",
    "    print(metric_name, \":\", metrics[metric_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the best model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best model to run predictions on the test data set (i.e. to predict whether an employee will leave the company).\n",
    "\n",
    "Print the first 10 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = x_test.pop(\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_test.head())\n",
    "#print(x_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = fitted_model.predict(x_test)\n",
    "print(y_predict[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve auto engineered features\n",
    "\n",
    "The `get_engineered_feature_names()` returns a list of engineered feature names.\n",
    "\n",
    "Note: Use `timeseriestransformer` for `task=forecasting`, else use `datatransformer` for `regression` or `classification` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.named_steps['datatransformer'].get_engineered_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_featurization_summary()` gets a featurization summary of all the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.named_steps['datatransformer'].get_featurization_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the engineered feature importances from the best run\n",
    "\n",
    "You can use [`ExplanationClient`](https://docs.microsoft.com/en-us/python/api/azureml-interpret/azureml.interpret.explanationclient?view=azure-ml-py) to download the engineered feature explanations from the artifact store of the `best_run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run(best_run)\n",
    "engineered_explanations = client.download_model_explanation(raw=False) #default raw=True when not specified.\n",
    "feature_importances = engineered_explanations.get_feature_importance_dict()\n",
    "\n",
    "# Overall feature importance\n",
    "print('Feature\\tImportance')\n",
    "for key, value in feature_importances.items():\n",
    "    print(key, '\\t', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the raw feature importances from the best run\n",
    "\n",
    "You can use `ExplanationClient` to download the raw feature explanations from the artifact store of the `best_run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run(best_run)\n",
    "raw_explanations = client.download_model_explanation() #default raw=True when not specified.\n",
    "feature_importances = raw_explanations.get_feature_importance_dict() \n",
    "\n",
    "# Overall feature importance\n",
    "print('Feature\\tImportance')\n",
    "for key, value in feature_importances.items():\n",
    "    print(key, '\\t', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model explanations in automated ML (Preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model explanations\n",
    "\n",
    "Use [`automl_setup_model_explanations`](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-runtime/azureml.train.automl.runtime.automl_explain_utilities?view=azure-ml-py#automl-setup-model-explanations-fitted-model--typing-union-sklearn-pipeline-pipeline--azureml-automl-runtime-streaming-pipeline-wrapper-streamingpipelinewrapper---task--str--x--typing-union-numpy-ndarray--pandas-core-frame-dataframe--scipy-sparse-base-spmatrix--azureml-dataprep-api-dataflow-dataflow--azureml-data-tabular-dataset-tabulardataset--nonetype----none--x-test--typing-union-numpy-ndarray--pandas-core-frame-dataframe--scipy-sparse-base-spmatrix--azureml-dataprep-api-dataflow-dataflow--azureml-data-tabular-dataset-tabulardataset--nonetype----none--y--typing-union-numpy-ndarray--pandas-core-series-series--pandas-core-arrays-categorical-categorical--azureml-dataprep-api-dataflow-dataflow--azureml-data-tabular-dataset-tabulardataset--nonetype----none--y-test--typing-union-numpy-ndarray--pandas-core-series-series--pandas-core-arrays-categorical-categorical--azureml-dataprep-api-dataflow-dataflow--azureml-data-tabular-dataset-tabulardataset--nonetype----none--features--typing-union-typing-list-str---nonetype----none--automl-run--typing-union-azureml-core-run-run--nonetype----none----kwargs--typing-any-----azureml-train-automl-runtime-automl-explain-utilities-automlexplainersetupclass) to get the engineered and raw explanations.\n",
    "                                        \n",
    "The `fitted_model` can generate the following items:\n",
    "- Featured data from trained or test samples\n",
    "- Engineered feature name lists\n",
    "- Findable classes in your labeled column in classification scenarios\n",
    "\n",
    "`automl_setup_model_explanations` returns an [`AutoMLExplainerSetupClass`](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-runtime/azureml.train.automl.runtime.automl_explain_utilities.automlexplainersetupclass?view=azure-ml-py) object that contains all the structures from above list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = x_train.pop(\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_train.head())\n",
    "#print(x_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl.runtime.automl_explain_utilities import automl_setup_model_explanations\n",
    "\n",
    "automl_explainer_setup_obj = automl_setup_model_explanations(fitted_model,\n",
    "                                                             X=x_train, \n",
    "                                                             X_test=x_test,\n",
    "                                                             y=y_train, \n",
    "                                                             task='classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Mimic Explainer for feature importance\n",
    "\n",
    "To generate an explanation for automated ML models, one way is to use the [`MimicWrapper`](https://docs.microsoft.com/en-us/python/api/azureml-interpret/azureml.interpret.mimic_wrapper.mimicwrapper?view=azure-ml-py) class. You can initialize the `MimicWrapper` with these parameters:\n",
    "- The explainer setup object\n",
    "- Your workspace\n",
    "- A surrogate model to explain the `fitted_model` automated ML model\n",
    "\n",
    "The `MimicWrapper` also takes the `best_run` object where the engineered explanations will be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.interpret import MimicWrapper\n",
    "\n",
    "# Initialize the Mimic Explainer to explain transformed features.\n",
    "explainer = MimicWrapper(ws, \n",
    "                         automl_explainer_setup_obj.automl_estimator,\n",
    "                         explainable_model=automl_explainer_setup_obj.surrogate_model, \n",
    "                         init_dataset=automl_explainer_setup_obj.X_transform,\n",
    "                         run=best_run,\n",
    "                         features=automl_explainer_setup_obj.engineered_feature_names, \n",
    "                         feature_maps=[automl_explainer_setup_obj.feature_map],\n",
    "                         classes=automl_explainer_setup_obj.classes,\n",
    "                         explainer_kwargs=automl_explainer_setup_obj.surrogate_model_params\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Mimic Explainer for computing and visualizing engineered feature importance\n",
    "\n",
    "You can call the `explain()` method in `MimicWrapper` with the transformed test samples to get the feature importance for the generated engineered features.\n",
    "\n",
    "You can also sign in to Azure Machine Learning studio to view the explanations dashboard visualization of the feature importance values of the generated engineered features by automated ML featurizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_explanations = explainer.explain(explanation_types=['local', 'global'],\n",
    "                                            eval_dataset=automl_explainer_setup_obj.X_test_transform)\n",
    "\n",
    "feature_importances = engineered_explanations.get_feature_importance_dict()\n",
    "\n",
    "# Overall feature importance\n",
    "print('Feature\\tImportance')\n",
    "for key, value in feature_importances.items():\n",
    "    print(key, '\\t', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the explanation results with `ExplanationDashboard` from `interpret-community` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install interpret-community[visualization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_community.widget import ExplanationDashboard\n",
    "\n",
    "ExplanationDashboard(engineered_explanations, \n",
    "                     automl_explainer_setup_obj.automl_estimator, \n",
    "                     datasetX=automl_explainer_setup_obj.X_test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the feature importance of the raw features by setting `get_raw=True` and specify `raw_feature_names` to display the feature name.\n",
    "\n",
    "Note: The dashboard visualization of the raw features can only be viewed in the Machine Learning studio. See [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-automated-ml-for-ml-models#model-explanations-preview) for the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_explanations = explainer.explain(explanation_types=['local', 'global'],\n",
    "                                     eval_dataset=automl_explainer_setup_obj.X_test_transform,\n",
    "                                     get_raw=True,\n",
    "                                     raw_feature_names=automl_explainer_setup_obj.raw_feature_names)\n",
    "\n",
    "feature_importances = raw_explanations.get_feature_importance_dict()\n",
    "\n",
    "# Overall feature importance\n",
    "print('Feature\\tImportance')\n",
    "for key, value in feature_importances.items():\n",
    "    print(key, '\\t', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register model\n",
    "\n",
    "To register the model from an automated ML run, use the register_model() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate how to use tags.\n",
    "tags = {}\n",
    "tags['Best AUC_weighted'] = local_run.get_metrics('AUC_weighted').get('AUC_weighted')\n",
    "print(tags)\n",
    "\n",
    "# Give a model name and description.\n",
    "model_name = 'predict-employee-retention-automl-model-' + best_run.properties['model_name']\n",
    "description = 'Predict employee retention best model from auto ML run.'\n",
    "model = local_run.register_model(model_name = model_name, \n",
    "                           description = description, \n",
    "                           tags = tags)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy your model without writing code\n",
    "\n",
    "1. Once the model is registered, you can find it in the studio by selecting Models on the left pane.\n",
    "\n",
    "\n",
    "2. Open the model, click the Deploy button at the top of the screen.\n",
    "\n",
    "\n",
    "See [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-automated-ml-for-ml-models#deploy-your-model) for the deployment steps."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "diray"
   }
  ],
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
