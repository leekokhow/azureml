{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #3: Create Azure Machine Learning Pipelines and model deployment.\n",
    "In this tutorial, we build a pipeline with data dependancy.\n",
    "\n",
    "The sequential steps are constructed in the pipeline as follows:\n",
    " 1. Prepare Data\n",
    " 2. Model Training\n",
    " 3. Evaluate Model\n",
    " 4. Register Model\n",
    "    \n",
    "The scripts created here can be used for setting up an [Azure MLOps](https://github.com/microsoft/MLOpsPython/blob/master/docs/getting_started.md).  \n",
    "       \n",
    "This tutorial also covers how to create a scoring script that receives data submitted to a deployed web service and passes it to the model, and  then takes the response returned by the model and returns that to the client.\n",
    "\n",
    "Finally, deploy the model and scoring script as Azure Container Instances webservice and Azure Kubernetes Service webservice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Setup your development environment and cloud compute resource\n",
    "\n",
    "This section is to check you have Azure ML SDK installed, has an existing Azure Machine Learning Workspace in the Azure portal, and be able to provision Azure compute resource in order to proceed with this tutorial.\n",
    "\n",
    "The Azure ML SDK version used here is 1.3.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Azure Machine Learning and Pipeline SDK-specific Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the packages that will be used in this tutorial.\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.webservice import AciWebservice, AksWebservice, Webservice\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type \\\n",
    "  import NumpyParameterType\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create or Attach existing compute resource\n",
    "\n",
    "If you don't have any compute target, you can run the following code to create one. Once it is completed, you can find 'cpucluster' in your Workspace under Compute &gt; Training clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_workspace = Workspace.from_config()\n",
    "datastore = aml_workspace.get_default_datastore()\n",
    "dataset_name = 'predict-employee-retention-training-data'\n",
    "experiment_name = 'predict-employee-retention'\n",
    "model_name = 'predict-employee-retention-model'\n",
    "file_name = 'training-data.csv'\n",
    "sources_directory_train = 'emp_retention'\n",
    "data_prep_script_path = 'training/prep_data.py'\n",
    "train_script_path = 'training/training_model.py'\n",
    "evaluate_script_path = 'evaluate/evaluate_model.py'\n",
    "register_script_path = 'register/register_model.py'\n",
    "score_script_path = 'scoring/score.py'\n",
    "aci_service_name = 'employee-retention-aci'\n",
    "aks_service_name = 'employee-retention-aks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Create scripts to use for the pipeline\n",
    "\n",
    "This section is to provide the necessary scripts to run in the pipeline steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 2)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_DS2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in aml_workspace.compute_targets:\n",
    "    compute_target = aml_workspace.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes,\n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(\n",
    "        aml_workspace, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())\n",
    "\n",
    "# Use \"cpucluster\"\n",
    "aml_compute = aml_workspace.compute_targets[\"cpucluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a folder to host scripts\n",
    "A folder structure is created to host the python scripts for each of the pipeline steps and the same\n",
    "structure will be copied to the compute target when the pipeline is executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must create a source_directory as the root folder.\n",
    "os.makedirs(os.path.join(os.getcwd(), sources_directory_train), exist_ok=True)\n",
    "\n",
    "# Create subfolders to host the scripts. This is optional because you can put everything in source_directory.\n",
    "training_folder = os.path.join(sources_directory_train,'training')\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "evaluate_folder = os.path.join(sources_directory_train,'evaluate')\n",
    "os.makedirs(evaluate_folder, exist_ok=True)\n",
    "\n",
    "register_folder = os.path.join(sources_directory_train,'register')\n",
    "os.makedirs(register_folder, exist_ok=True)\n",
    "\n",
    "scoring_folder = os.path.join(sources_directory_train,'scoring')\n",
    "os.makedirs(scoring_folder, exist_ok=True)\n",
    "\n",
    "print('Scripts used for the pipeline will be placed in the following directory:', \n",
    "      training_folder, evaluate_folder, register_folder, scoring_folder, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create script to Prepare Data\n",
    "This step uses an input .csv file (i.e. the raw data) and produce an output .csv file (i.e. clean_data.csv) that is stored in the DataStore. The clean data is an intermediate data to be consumed by subsequent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prep_data.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset, run):\n",
    "    # Rename sales feature into department\n",
    "    dataset = dataset.rename(columns={\"sales\": \"department\"})\n",
    "\n",
    "    # Map salary into integers\n",
    "    salary_map = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "    dataset[\"salary\"] = dataset[\"salary\"].map(salary_map)\n",
    "\n",
    "    # Create dummy variables for department feature\n",
    "    dataset = pd.get_dummies(dataset, columns=[\"department\"],\n",
    "                             drop_first=True)\n",
    "\n",
    "    # Get number of positve and negative examples\n",
    "    pos = dataset[dataset[\"left\"] == 1].shape[0]\n",
    "    neg = dataset[dataset[\"left\"] == 0].shape[0]\n",
    "    run.log('Positive', 'Positive examples = {}'.format(pos))\n",
    "    run.log('Negative', 'Negative examples = {}'.format(neg))\n",
    "    run.log('Proportion', 'Proportion of positive to negative \\\n",
    "    examples = {:.2f}%'.format((pos / neg) * 100))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def main():\n",
    "    # retrieve argument configured through script_params in estimator\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--raw_data_file', dest='raw_data_file',\n",
    "                        type=str, help='input raw data in .csv file')\n",
    "    parser.add_argument('--clean_data_folder', dest='clean_data_folder',\n",
    "                        type=str, help='output folder path that stores \\\n",
    "                        the clean data file')\n",
    "    parser.add_argument('--clean_data_file', dest='clean_data_file',\n",
    "                        type=str, help='name of the clean data file')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # get hold of the current run\n",
    "    run = Run.get_context()\n",
    "\n",
    "    # Read dataset\n",
    "    dataset = pd.read_csv(args.raw_data_file)\n",
    "    run.log('Read raw data file', args.raw_data_file)\n",
    "\n",
    "    # Clean the dataset to use for model training\n",
    "    dataset = prepare_dataset(dataset, run)\n",
    "\n",
    "    # For \"output\" PipelineData, the folder must be created using\n",
    "    # os.makedirs() first, then only can write files into the folder.\n",
    "    os.makedirs(args.clean_data_folder, exist_ok=True)\n",
    "\n",
    "    # Write the dataset into .csv file for the next step to process\n",
    "    dataset.to_csv(args.clean_data_folder + args.clean_data_file, index=False)\n",
    "    run.log('Output path of clean data', args.clean_data_folder\n",
    "            + args.clean_data_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create script for Model Training\n",
    "\n",
    "This step uses the clean data to train a model and then store the model (.pkl) file into the DataStore. The model is an intermediate data to be consumed by subsequent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training_model.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def generate_model(dataset, run):\n",
    "    # Convert dataframe into numpy objects and split them into\n",
    "    # train and test sets: 80/20\n",
    "    X = dataset.loc[:, dataset.columns != \"left\"].values\n",
    "    y = dataset.loc[:, dataset.columns == \"left\"].values.flatten()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=1)\n",
    "\n",
    "    clf = LogisticRegression(solver='liblinear', random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # View the model's coefficients and bias\n",
    "    run.log('Coefficients', clf.coef_)\n",
    "    run.log('Bias', clf.intercept_)\n",
    "\n",
    "    y_pred_LR = clf.predict(X_test)\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred_LR)\n",
    "    group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "    labels = [f\"{v1} {v2} ({v3})\" for v1, v2, v3 in\n",
    "              zip(group_names, group_counts, group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2, 2)\n",
    "    run.log('Confusion Matrix', labels)\n",
    "\n",
    "    # Display statistics\n",
    "    accuracy = np.trace(cf_matrix) / float(np.sum(cf_matrix))\n",
    "    precision = cf_matrix[1, 1] / sum(cf_matrix[:, 1])\n",
    "    recall = cf_matrix[1, 1] / sum(cf_matrix[1, :])\n",
    "    f1_score = 2*precision*recall / (precision + recall)\n",
    "\n",
    "    stats_text = \"\\n\\nAccuracy={:0.3f}\\n\".format(accuracy) + \\\n",
    "        \"Precision={:0.3f}\\n\".format(precision) + \\\n",
    "        \"Recall={:0.3f}\\n\".format(recall) + \\\n",
    "        \"F1 Score={:0.3f}\".format(f1_score)\n",
    "    run.log('Statistics', stats_text)\n",
    "\n",
    "    # Log the following metrics to the parent run so that\n",
    "    # these are available for model evaluation later.\n",
    "    run.parent.log('Accuracy', accuracy)\n",
    "    run.parent.log('Precision', precision)\n",
    "    run.parent.log('Recall', recall)\n",
    "    run.parent.log('F1-score', f1_score)\n",
    "\n",
    "    # Log confusion matrix as JSON.\n",
    "    cf_matrix_json = {\"schema_type\": \"confusion_matrix\",\n",
    "                      \"schema_version\": \"v1\",\n",
    "                      \"data\": {\"class_labels\": group_names,\n",
    "                               \"matrix\": cf_matrix.tolist()}}\n",
    "\n",
    "    run.log_confusion_matrix('Employee Retention Confusion Matrix',\n",
    "                             cf_matrix_json,\n",
    "                             description='Confusion matrix generated \\\n",
    "                             for the run')\n",
    "    return clf\n",
    "\n",
    "\n",
    "def main():\n",
    "    # retrieve argument configured through script_params in estimator\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--clean_data_folder', dest='clean_data_folder',\n",
    "                        type=str,\n",
    "                        help='folder path that stores the clean data file')\n",
    "    parser.add_argument('--new_model_folder', dest='new_model_folder',\n",
    "                        type=str,\n",
    "                        help='output folder path that stores the new \\\n",
    "                        model .pkl file name')\n",
    "    parser.add_argument('--clean_data_file', dest='clean_data_file',\n",
    "                        type=str,\n",
    "                        help='name of the clean data file')\n",
    "    parser.add_argument('--new_model_file', dest='new_model_file',\n",
    "                        type=str,\n",
    "                        help='name of the new model .pkl file')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # get the current run\n",
    "    run = Run.get_context()\n",
    "\n",
    "    # Read dataset\n",
    "    training_dataset_file = args.clean_data_folder + args.clean_data_file\n",
    "    dataset = pd.read_csv(training_dataset_file)\n",
    "    run.log('Read training data from file', training_dataset_file)\n",
    "\n",
    "    # Generate model\n",
    "    clf = generate_model(dataset, run)\n",
    "\n",
    "    # For \"output\" PipelineData, the folder must be created using\n",
    "    # os.makedirs() first, then only can write files into the folder.\n",
    "    os.makedirs(args.new_model_folder, exist_ok=True)\n",
    "\n",
    "    # Pass model file to next step\n",
    "    model_pkl_file = args.new_model_folder + args.new_model_file\n",
    "    joblib.dump(value=clf, filename=model_pkl_file)\n",
    "\n",
    "    run.log('Output path of new model', model_pkl_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create script to Evaluate Model\n",
    "\n",
    "This step will compare the new model's metrics with the current model. It retrieves the current model from the Azure ML Workspace.\n",
    "\n",
    "After the new model is evaluated to perform better than the current one, the next step will continue to register the new model. Otherwise, the pipeline will stop at this step and cancel the run so that the remaining steps will not get executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate_model.py\n",
    "\n",
    "import argparse\n",
    "from azureml.core import Run\n",
    "from azureml.core.model import Model\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "\n",
    "def main():\n",
    "    # retrieve argument configured through script_params in estimator\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_name\", dest='model_name', type=str,\n",
    "                        help=\"Name of the model to retrieve from Workspace\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Get the current run\n",
    "    run = Run.get_context()\n",
    "\n",
    "    # Get metrics from current model and compare with the metrics\n",
    "    # of the new model. The metrics of the new model can be retrieved\n",
    "    # from run.parent.get_metrics, which were created in training_model.py\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
    "    current_metrics = {}\n",
    "    new_metrics = {}\n",
    "\n",
    "    try:\n",
    "        workspace = run.experiment.workspace\n",
    "        # Get latest model\n",
    "        model = Model(workspace, args.model_name)\n",
    "\n",
    "        for key in metrics:\n",
    "            current_metrics[key] = float(model.tags.get(key))\n",
    "            new_metrics[key] = run.parent.get_metrics(key).get(key)\n",
    "            run.log(key, 'current(ver '\n",
    "                         + str(model.version)\n",
    "                         + ')='\n",
    "                         + model.tags.get(key)\n",
    "                         + ' new='\n",
    "                         + str(run.parent.get_metrics(key).get(key))\n",
    "                    )\n",
    "\n",
    "    except WebserviceException as e:\n",
    "        if('ModelNotFound' in e.message):\n",
    "            model = None\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # Perform comparison. Just do a simple comparison:\n",
    "    # If Accuracy improves, proceed next step to register model.\n",
    "    if(model is not None):\n",
    "        if(new_metrics['Accuracy'] >= current_metrics['Accuracy']):\n",
    "            run.log(\"Result\", \"New Accuracy is as good as current, \\\n",
    "                will proceed to register new model.\")\n",
    "        else:\n",
    "            run.log(\"Result\", \"New Accuracy is worse than current, \\\n",
    "                will not register model. Processing cancelled.\")\n",
    "            run.parent.cancel()\n",
    "    else:\n",
    "        run.log(\"Result\", \"This is the first model, will proceed \\\n",
    "            to register the model.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create script to Register Model\n",
    "\n",
    "This step is to register the new model that was created after the model training. It uses the intermediate output (i.e. model (.pkl) file) as an input.\n",
    "\n",
    "Note: In Tutorial #1, we use **run.register_model()** to register the model:\n",
    "\n",
    "model = run.register_model(model_name='predict-employee-retention',\n",
    "                           model_path='outputs/predict-employee-retention-model.pkl',\n",
    "                           tags=tags\n",
    "                          )\n",
    "\n",
    "**run.register_model()** will not work here, so you need to use **Model.register()** as shown in the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile register_model.py\n",
    "\n",
    "import argparse\n",
    "from azureml.core import Run, Dataset\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Retrieve argument configured through script_params in estimator\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--new_model_folder', dest='new_model_folder',\n",
    "                        type=str,\n",
    "                        help='input folder path for reading the new \\\n",
    "                        model .pkl file')\n",
    "    parser.add_argument('--new_model_file', dest='new_model_file',\n",
    "                        type=str,\n",
    "                        help='name of the model .pkl file')\n",
    "    parser.add_argument(\"--model_name\", dest='model_name',\n",
    "                        type=str,\n",
    "                        help=\"Name of the model to register into \\\n",
    "                        Azure ML Workspace\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Get the current run\n",
    "    run = Run.get_context()\n",
    "    # Adding metrics to tags so that these information can\n",
    "    # be used for model comparison purpose.\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
    "    tags = {}\n",
    "    for key in metrics:\n",
    "        tags[key] = run.parent.get_metrics(key).get(key)\n",
    "\n",
    "    # Store BuildId\n",
    "    parent_tags = run.parent.get_tags()\n",
    "    build_id = 'BuildId'\n",
    "    try:\n",
    "        build_id = parent_tags[\"BuildId\"]\n",
    "        tags['BuildId'] = build_id\n",
    "    except KeyError:\n",
    "        print(\"BuildId tag not found on parent run.\")\n",
    "        print(f\"Tags present: {parent_tags}\")\n",
    "\n",
    "    # Register the new model, note the metric values are stored in \"tags\".\n",
    "    model_pkl_file = args.new_model_folder + args.new_model_file\n",
    "    workspace = run.experiment.workspace\n",
    "    dataset_name = 'predict-employee-retention-training-data'\n",
    "    model = Model.register(workspace=workspace,\n",
    "                           model_name=args.model_name,\n",
    "                           model_path=model_pkl_file,\n",
    "                           tags=tags,\n",
    "                           datasets=[('training data',\n",
    "                                      Dataset.get_by_name(workspace,\n",
    "                                                          dataset_name))])\n",
    "\n",
    "    run.log('Model registered', 'New model ' + model.name\n",
    "            + ' version ' + str(model.version) + ' BuildId ' + build_id)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Copy scripts to subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy('prep_data.py', training_folder)\n",
    "shutil.copy('training_model.py', training_folder)\n",
    "shutil.copy('evaluate_model.py', evaluate_folder)\n",
    "shutil.copy('register_model.py', register_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Prepare data source to use in the pipeline\n",
    "\n",
    "### 1. Create a DataReference object to reference the data file in the datastore.\n",
    "\n",
    "Datasource is represented by **[DataReference](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.data_reference.datareference?view=azure-ml-py)** object and points to data that lives in or is accessible from Datastore. DataReference could be a pointer to a file or a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the \"training-data.csv\" data file to datastore.\n",
    "datastore.upload_files([file_name],\n",
    "                       target_path=dataset_name,\n",
    "                       overwrite=True)\n",
    "\n",
    "raw_data_file = DataReference(datastore=datastore,\n",
    "    data_reference_name=\"Raw_Data_File\",\n",
    "    path_on_datastore=dataset_name + '/' + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check your uploaded data file in the datastore\n",
    "Once you successfully uploaded the files, you can browse to them (or upload more files).\n",
    "\n",
    "- In the [**Azure Portal**](https://portal.azure.com), go to your Machine Learning Workspace. \n",
    "\n",
    "\n",
    "- In your Workspace, click on the \"Storage\" hyperlink. This will bring you to the storage account page.\n",
    "\n",
    "\n",
    "- In the storage account page, click \"Storage Explorer\" on the left-hand menu.\n",
    "\n",
    "\n",
    "- You will find the uploaded file in BLOB CONTAINERS > azureml-blobstore-[your subscription ID] > predict-employee-retention-training-data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Building Pipeline Steps with Inputs and Outputs\n",
    "A step in the pipeline can take data as input. This data can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline. This tutorial illustrates how to use PythonScriptStep and EstimatorStep to build the pipeline.\n",
    "\n",
    "Intermediate data (or output of a Step) is represented by **[PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py)** object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps.\n",
    "\n",
    "[**PipelineParameter**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineparameter?view=azure-ml-py) can be used to pass varying parameter values to a published Pipeline.\n",
    "\n",
    "A [**DataPath**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.datapath.datapath?view=azure-ml-py) can be modified during pipeline submission with the PipelineParameter.\n",
    "\n",
    "Tips: \n",
    "1. If you use a **PipelineData** object in *arguments*, you must also specify it in either *inputs* or *outputs*.\n",
    "\n",
    "\n",
    "2. For **PipelineParameter** object use in *arguments*, you don't have to specify it in *inputs* or *outputs*.\n",
    "\n",
    "\n",
    "3. If you use a **DataReference** object in *arguments*, you must specify it in *inputs*.\n",
    "\n",
    "\n",
    "4. When use **PipelineData** as *outputs* in a pipeline step, if this step is the first and will write output data, you must use os.makedirs() to create the **PipelineData** folder first then only you are able to write files to the folder. See [Moving data into and between ML pipeline steps (Python)](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-move-data-in-out-of-pipelines#use-pipelinedata-for-intermediate-data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Prepare Data Step\n",
    "\n",
    "The first step in the pipeline is to \"clean\" the data and subsequently pass it to the second step to perform model training.\n",
    "The [**PythonScriptStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py) can be used here.\n",
    "Notice this step will run the 'training/prep_data.py' python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PipelineParameter and PipelineData used for \n",
    "# prepare data step.\n",
    "# clean_data_folder is used to store the clean data file.\n",
    "# Take note when create PipelineData, you don't need to \n",
    "# specify the full folder path, just need the name of the subfolder \n",
    "# to be created e.g. \"clean_data_folder\"\n",
    "clean_data_file = PipelineParameter(name=\"clean_data_file\",\n",
    "                                    default_value=\"/clean_data.csv\")\n",
    "clean_data_folder = PipelineData(\"clean_data_folder\",\n",
    "                                 datastore=datastore)\n",
    "\n",
    "# raw_data_file is a Datareference and produce clean data to \n",
    "# be used for model training.\n",
    "prepDataStep = PythonScriptStep(name=\"Prepare Data\",\n",
    "                   source_directory=sources_directory_train,\n",
    "                   script_name=data_prep_script_path,\n",
    "                   arguments=[\"--raw_data_file\", raw_data_file,\n",
    "                              \"--clean_data_folder\", clean_data_folder,\n",
    "                              \"--clean_data_file\", clean_data_file],\n",
    "                   inputs=[raw_data_file],\n",
    "                   outputs=[clean_data_folder],\n",
    "                   compute_target=aml_compute,\n",
    "                   allow_reuse=False)\n",
    "\n",
    "print(\"Step Prepare Data created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Model Training Step\n",
    "\n",
    "This step is to perform model training using Scikit-Learn. This example shows you how to create using PythonScriptStep and EstimatorStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PipelineParameter and PipelineData used\n",
    "# for model training step.\n",
    "# new_model_folder is used to store the model .pkl file.\n",
    "new_model_file = PipelineParameter(name=\"new_model_file \",\n",
    "                                   default_value='/' + model_name\n",
    "                                   + '.pkl')\n",
    "new_model_folder = PipelineData(\"new_model_folder\", datastore=datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Using EstimatorStep\n",
    "[**EstimatorStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.estimator_step.estimatorstep?view=azure-ml-py) adds a step to run Estimator in a Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Estimator (in this case we use the SKLearn estimator)\n",
    "est = SKLearn(source_directory=sources_directory_train,\n",
    "              entry_script=train_script_path,\n",
    "              # Use either pip_packages or conda_packages.\n",
    "              pip_packages=['azureml-sdk', 'scikit-learn==0.20.3', \\\n",
    "                            'azureml-dataprep[pandas,fuse]>=1.1.14'],\n",
    "              #conda_packages=['scikit-learn==0.20.3'],\n",
    "              compute_target=aml_compute)\n",
    "\n",
    "# Notice here you can't use PipelineParameter object, \n",
    "# but you can specify its value to pass.\n",
    "trainingStep = EstimatorStep(\n",
    "    name=\"Model Training\", \n",
    "    estimator=est,\n",
    "    estimator_entry_script_arguments=[\"--clean_data_folder\", \\\n",
    "                                      clean_data_folder, \\\n",
    "                                      \"--new_model_folder\", \\\n",
    "                                      new_model_folder,\n",
    "                                      \"--clean_data_file\", \\\n",
    "                                      clean_data_file.default_value,\n",
    "                                      \"--new_model_file\", \\\n",
    "                                      new_model_file.default_value],\n",
    "    runconfig_pipeline_params=None, \n",
    "    inputs=[clean_data_folder], \n",
    "    outputs=[new_model_folder], \n",
    "    compute_target=aml_compute,\n",
    "    allow_reuse=False)\n",
    "\n",
    "print(\"Step Train created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Using PythonScriptStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a RunConfiguration to specify additional requirements for this step, like the specific packages required.\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn==0.20.3'])\n",
    "\n",
    "trainingStep = PythonScriptStep(\n",
    "    name=\"Model Training\",\n",
    "    source_directory=sources_directory_train,\n",
    "    script_name=train_script_path, \n",
    "    arguments=[\"--clean_data_folder\", clean_data_folder, \n",
    "        \"--new_model_folder\", new_model_folder,\n",
    "        \"--clean_data_file\", clean_data_file,\n",
    "        \"--new_model_file\", new_model_file],\n",
    "    inputs=[clean_data_folder],\n",
    "    outputs=[new_model_folder],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=run_config,\n",
    "    allow_reuse=False)\n",
    "\n",
    "print(\"Step Train created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Model Evaluation Step\n",
    "\n",
    "This step is to evaluate the new model, whether it is better than the current model.\n",
    "\n",
    "\n",
    "Using the *model_name* as input to the python code *'evaluate/evaluate_model.py'*, \n",
    "this step will fetch the current model from Azure ML Workspace and then compare its metrics against the new model.\n",
    "You write your comparison logic in *'evaluate/evaluate_model.py'*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PipelineParameter to pass the name of the model to be evaluated.\n",
    "model_name_param = PipelineParameter(name=\"model_name\",\n",
    "                                     default_value=model_name)\n",
    "\n",
    "evaluateStep = PythonScriptStep(\n",
    "    name=\"Evaluate Model\",\n",
    "    source_directory=sources_directory_train,\n",
    "    script_name=evaluate_script_path, \n",
    "    arguments=[\"--model_name\", model_name_param],\n",
    "    compute_target=aml_compute,\n",
    "    allow_reuse=False)\n",
    "\n",
    "print(\"Step Evaluate created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Model Registration Step\n",
    "\n",
    "After evaluated the new model is better, this step will register it into the Azure ML Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registerStep = PythonScriptStep(\n",
    "    name=\"Register Model\",\n",
    "    source_directory=sources_directory_train,\n",
    "    script_name=register_script_path, \n",
    "    arguments=[\"--new_model_folder\", new_model_folder,\n",
    "               \"--new_model_file\", new_model_file,\n",
    "               \"--model_name\", model_name_param],\n",
    "    inputs=[new_model_folder],\n",
    "    compute_target=aml_compute,\n",
    "    allow_reuse=False)\n",
    "\n",
    "print(\"Step Register created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Define the Pipeline Steps\n",
    "\n",
    "You can define the steps to be run concurrently or sequentially. This example is to show how to chain the steps in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain the steps in sequence.\n",
    "trainingStep.run_after(prepDataStep)\n",
    "evaluateStep.run_after(trainingStep)\n",
    "registerStep.run_after(evaluateStep)\n",
    "\n",
    "pipeline = Pipeline(workspace=aml_workspace, steps=[registerStep])\n",
    "pipeline.validate()\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Run the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Submit the pipeline\n",
    "[Submitting](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py#submit) the pipeline involves creating an [Experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment?view=azure-ml-py) object and providing the built pipeline for submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(aml_workspace, experiment_name).submit(pipeline)\n",
    "\n",
    "# Use RunDetails Widget to examine the run of the pipeline. \n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. View Pipeline outputs\n",
    "\n",
    "See where outputs of each pipeline step are located on your datastore.\n",
    "\n",
    "Wait for pipeline run to complete, to make sure all the outputs are ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each step to get the list of outputs.\n",
    "for step in pipeline_run.get_steps():\n",
    "    print(\"Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "    \n",
    "    for name, output in output_dict.items():\n",
    "        \n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get additonal run details\n",
    "If you wait until the pipeline_run is finished, you may be able to get additional details on the run by executing the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion()\n",
    "for step_run in pipeline_run.get_children():\n",
    "    print(\"{}: {}\".format(step_run.name, step_run.get_metrics()), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Publish the Pipeline\n",
    "\n",
    "The published pipeline can be found in Azure Machine Learning studio > Pipelines > Pipeline endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: from a submitted PipelineRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timenow = datetime.now().strftime('%m-%d-%Y-%H-%M')\n",
    "pipeline_name = timenow + \"-Employee Retention Pipeline\"\n",
    "print(pipeline_name)\n",
    "\n",
    "published_pipeline1 = pipeline_run.publish_pipeline(\n",
    "    name=pipeline_name, \n",
    "    description=\"Predict Employee Retention Model training/retraining pipeline (from submitted PipelineRun)\", \n",
    "    version=\"0.1\", \n",
    "    continue_on_step_failure=True)\n",
    "\n",
    "print(\"Newly published pipeline id: {}\".format(published_pipeline1.id))\n",
    "\n",
    "published_pipeline1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the *continue_on_step_failure* parameter specifies whether the execution of steps in the Pipeline will continue if one step fails. The default value is False, meaning when one step fails, the Pipeline execution will stop, canceling any running steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: from a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timenow = datetime.now().strftime('%m-%d-%Y-%H-%M')\n",
    "pipeline_name = timenow + \"-Employee Retention Pipeline\"\n",
    "print(pipeline_name)\n",
    "\n",
    "published_pipeline2 = pipeline.publish(\n",
    "    name=pipeline_name, \n",
    "    description=\"Predict Employee Retention Model training/retraining pipeline (from Pipeline)\",\n",
    "    version=\"0.2\",\n",
    "    continue_on_step_failure=True)\n",
    "\n",
    "print(\"Newly published pipeline id: {}\".format(published_pipeline2.id))\n",
    "\n",
    "published_pipeline2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. Run published pipeline using its REST endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "#rest_endpoint = published_pipeline1.endpoint\n",
    "rest_endpoint = published_pipeline2.endpoint\n",
    "\n",
    "print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint))\n",
    "\n",
    "# specify the param when running the pipeline\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"predict_employee_retention_pipeline\",\n",
    "                               \"RunSource\": \"SDK\",\n",
    "                               \"ParameterAssignments\": {\"pipeline_arg\": 45}})\n",
    "run_id = response.json()[\"Id\"]\n",
    "\n",
    "print('Submitted pipeline run: ', run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see the submitted pipeline (given by the run_id) in **Azure ML studio > Experiments > predict_employee_retention_pipeline > click on the Run ID**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I. Create a scoring script\n",
    "\n",
    "This tutorial covers how to create a scoring script that receives data submitted to a deployed web service and passes it to the model, and  then takes the response returned by the model and returns that to the client. \n",
    "This script is specific to your model.\n",
    "\n",
    "[Deploy models with Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test model scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_path=Model(aml_workspace, model_name)\n",
    "model_path.download(target_dir=os.getcwd(), exist_ok=True)\n",
    "model_path_pkl = os.path.join(os.getcwd(), model_name+'.pkl')\n",
    "model = joblib.load(model_path_pkl)\n",
    "\n",
    "raw_data1 = '{\"data\": [[0.1, 0.77, 6.0, 272.0, 4.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}'\n",
    "\n",
    "raw_data2 = '{\"data\": [[0.76, 0.5, 4.0, 136.0, 3.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]]}'\n",
    "\n",
    "\n",
    "data1 = np.array(json.loads(raw_data1)['data'])\n",
    "y_hat1 = model.predict(data1) \n",
    "print(\"prediction:\", {\"result\": y_hat1.tolist()}) # Prediction result 1 (i.e. leave company)\n",
    "\n",
    "data2 = np.array(json.loads(raw_data2)['data'])\n",
    "y_hat2 = model.predict(data2) \n",
    "print(\"prediction:\", {\"result\": y_hat2.tolist()}) # Prediction result 0 (i.e. stay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install inference-schema\n",
    "\n",
    "This example is to illustrate how to use the automatic schema generation.\n",
    "\n",
    "Before you proceed, run the following pip commands to download this package into your local Anaconda:\n",
    "    \n",
    "+ pip install inference-schema[numpy-support]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create scoring script\n",
    "\n",
    "This section is to create a scoring script for your model.\n",
    "\n",
    "[Deploy models with Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where)\n",
    "\n",
    "#### Specify the environment to run the script and save it as conda_dependencies.yml\n",
    "\n",
    "* You must indicate azureml-defaults as a pip dependency, because it contains the functionality needed to host the model as a web service.\n",
    "* If you want to use automatic schema generation, your entry script must also import the inference-schema packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create conda dependencies in YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CondaDependencies.create(pip_packages=['azureml-sdk','azureml-defaults','scikit-learn==0.20.3','inference-schema[numpy-support]'])\n",
    "\n",
    "with open(\"conda_dependencies.yml\",\"w\") as f:\n",
    "    f.write(env.serialize_to_string())\n",
    "    \n",
    "# Review the content of the YAML file you just created.\n",
    "with open(\"conda_dependencies.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your scoring script and save it as score.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core.model import Model\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type \\\n",
    "  import NumpyParameterType\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # retrieve the path to the model file using the model name\n",
    "    model_path = Model.get_model_path('predict-employee-retention-model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "input_sample = np.array([[0.76, 0.5, 4.0, 136.0, 3.0, 0.0, 0.0, 1.0,\n",
    "                          0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]])\n",
    "output_sample = np.array([1])\n",
    "\n",
    "\n",
    "@input_schema('data', NumpyParameterType(input_sample))\n",
    "@output_schema(NumpyParameterType(output_sample))\n",
    "def run(data):\n",
    "    y_hat = model.predict(data)\n",
    "    return y_hat.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy your script to the scoring folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy('conda_dependencies.yml', scoring_folder)\n",
    "shutil.copy('score.py', scoring_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## J. Deploy as a Azure Container Instances webservice\n",
    "\n",
    "[What is Azure Container Instances?](https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create and deploy ACI webservice\n",
    "\n",
    "This step may take a while to start after you run the cell, you will see the message \"Running\" appearing when it starts and will take few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model=Model(aml_workspace, model_name)\n",
    "\n",
    "# Load the environment file.\n",
    "env = Environment.from_conda_specification(name = 'myenv', \n",
    "                                           file_path = scoring_folder+'/conda_dependencies.yml')\n",
    "\n",
    "# Combine scoring script & environment in Inference configuration\n",
    "inference_config = InferenceConfig(entry_script=sources_directory_train+'/'+score_script_path, environment=env)\n",
    "\n",
    "# Set deployment configuration.\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1,\n",
    "                                               tags={\"data\": \"Predict employee retention\",  \"method\" : \"sklearn\"}, \n",
    "                                               description='Predict employee retention with sklearn')\n",
    "\n",
    "# Define the model, inference, & deployment configuration and web service name and location to deploy\n",
    "aci_service = Model.deploy(\n",
    "    workspace = aml_workspace,\n",
    "    name = aci_service_name,\n",
    "    models = [model],\n",
    "    inference_config = inference_config,\n",
    "    deployment_config = deployment_config)\n",
    "\n",
    "aci_service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View status and logs (for troubleshooting purpose):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aci_service.state)\n",
    "print(aci_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the webservice is deployed successfully, you can use \n",
    "# the HTTP endpoint \"scoring_uri\" to test model scoring. \n",
    "aci_service = Webservice(workspace=aml_workspace, name=aci_service_name)\n",
    "print(aci_service.scoring_uri)\n",
    "print(aci_service.swagger_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test deployed ACI webservice\n",
    "\n",
    "Send HTTP request to test the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "# Data for scoring\n",
    "raw_data1 = '{\"data\": [[0.1, 0.77, 6.0, 272.0, 4.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}'\n",
    "resp = requests.post(aci_service.scoring_uri, raw_data1, headers=headers)\n",
    "print(\"POST to url\", aci_service.scoring_uri)\n",
    "print(\"input data:\", raw_data1)\n",
    "print(\"prediction:\", resp.text) # Return 1 (i.e. predict the employee will leave the organization)\n",
    "\n",
    "raw_data2 = '{\"data\": [[0.76, 0.5, 4.0, 136.0, 3.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]]}'\n",
    "resp = requests.post(aci_service.scoring_uri, raw_data2, headers=headers)\n",
    "print(\"POST to url\", aci_service.scoring_uri)\n",
    "print(\"input data:\", raw_data2)\n",
    "print(\"prediction:\", resp.text) # Return 0 (i.e. predict the employee will stay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Delete the ACI Webservice\n",
    "\n",
    "You can delete the ACI deployment using this API call if it is no longer required for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also manually delete the deployed web service which can be found in your **Azure ML Workspace > Deployments**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K. Deploy as a Azure Kubernetes Service webservice\n",
    "\n",
    "[Azure Kubernetes Service (AKS)](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create the AKS cluster\n",
    "\n",
    "This step may take a while to start after you run the cell, you will see the message \"Creating\" appearing when it starts and will take few minutes to complete.\n",
    "\n",
    "[Create a new AKS cluster](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service#create-a-new-aks-cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "\n",
    "# Create a dev/test cluster for testing purpose:\n",
    "prov_config = AksCompute.provisioning_configuration(cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST)\n",
    "\n",
    "aks_name = 'aks'\n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(workspace = aml_workspace,\n",
    "                                    name = aks_name,\n",
    "                                    provisioning_configuration = prov_config)\n",
    "\n",
    "# Wait for the create process to complete\n",
    "aks_target.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create and deploy AKS webservice\n",
    "\n",
    "This step may take a while to start after you run the cell, you will see the message \"Running\" appearing when it starts and will take few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the web service configuration (using default here)\n",
    "aks_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "aks_service = Model.deploy(workspace=aml_workspace,\n",
    "                           name=aks_service_name,\n",
    "                           models=[model],\n",
    "                           inference_config=inference_config,\n",
    "                           deployment_config=aks_config,\n",
    "                           deployment_target=aks_target)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the webservice is deployed successfully, you can use \n",
    "# the HTTP endpoint \"scoring_uri\" to test model scoring. \n",
    "aks_service = Webservice(workspace=aml_workspace, name=aks_service_name)\n",
    "print(aci_service.scoring_uri)\n",
    "print(aci_service.swagger_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test deployed AKS webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Need to get the authorization key, either one can be used.\n",
    "primary_key, secondary_key = aks_service.get_keys()\n",
    "headers = {'Content-Type':'application/json',  'Authorization':('Bearer '+ primary_key)} \n",
    "\n",
    "# Data for scoring\n",
    "raw_data1 = '{\"data\": [[0.1, 0.77, 6.0, 272.0, 4.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}'\n",
    "resp = requests.post(aks_service.scoring_uri, raw_data1, headers=headers)\n",
    "print(\"POST to url\", aks_service.scoring_uri)\n",
    "print(\"input data:\", raw_data1)\n",
    "print(\"prediction:\", resp.text) # Return 1 (i.e. predict the employee will leave the organization)\n",
    "\n",
    "raw_data2 = '{\"data\": [[0.76, 0.5, 4.0, 136.0, 3.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]]}'\n",
    "resp = requests.post(aks_service.scoring_uri, raw_data2, headers=headers)\n",
    "print(\"POST to url\", aks_service.scoring_uri)\n",
    "print(\"input data:\", raw_data2)\n",
    "print(\"prediction:\", resp.text) # Return 0 (i.e. predict the employee will stay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Delete the AKS Webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also manually delete the deployed web service which can be found in your **Azure ML Workspace > Deployments**."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "diray"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
