{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #1: Model training with Azure Machine Learning\n",
    "\n",
    "In this tutorial, you will train a machine learning model on local and Azure compute resources. You will explore the Azure Machine Learning service and the Azure ML SDK for Python. \n",
    "This notebook serves as a quick start to hands-on Azure Machine Learning service. \n",
    "\n",
    "The codes here were tested using Azure ML SDK version:\n",
    "- 1.6.0\n",
    "- 1.3.0\n",
    "- 1.0.72 on Microsoft Azure Notebooks with Python 3.6 kernel \n",
    "\n",
    "The use case is based on  \n",
    "* [Predicting Employee Turnover](https://towardsdatascience.com/predicting-employee-turnover-7ab2b9ecf47e) by Imad Dabbura \n",
    "* [Predict Employee Retention](https://towardsdatascience.com/predict-employee-retention-901bfb2c8db5) by Ila Maheshwari\n",
    "\n",
    "**Note: The focus of this tutorial is not about accuracy of the model or what algorithm to use. You can choose any use case that you want to explore.**\n",
    "\n",
    "Before you start this tutorial, you need to create a workspace in the Azure portal first.\n",
    "[Create and manage Azure Machine Learning workspaces in the Azure portal](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace)\n",
    "\n",
    "The following are covered in this tutorial:\n",
    "* Store a dataset in the Azure Machine Learning datastore\n",
    "* Retrieve a dataset from the Azure Machine Learning datastore\n",
    "* Train a simple logistic regression model on local machine and on Azure compute resources\n",
    "* Register the model in Azure Machine Learning Workspace\n",
    "\n",
    "If you are trying out this tutorial for the first time, please run the code cells in this tutorial sequentially.\n",
    "Tutorial #2 will cover the basics of deploying a model. \n",
    "\n",
    "## References\n",
    "\n",
    "[How Azure Machine Learning works: Architecture and concepts](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture)\n",
    "\n",
    "[Azure Machine Learning documentation](https://docs.microsoft.com/azure/machine-learning/service/tutorial-train-models-with-aml#prerequisites).\n",
    "                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your development environment\n",
    "\n",
    "### Dependencies required for local machine setup in order to use Azure ML SDK.\n",
    "\n",
    "Step 1. You need to create a [free Azure account](https://azure.microsoft.com/en-gb/free/) first. This tutorial will use Anaconda on your local machine to connect to your Azure account.\n",
    "\n",
    "Step 2. This notebook was tested in Anaconda Jupyter Notebook. \n",
    "Once you have installed Anaconda on your machine, run the following pip commands to download these packages into Anaconda:\n",
    "    \n",
    "+ conda install anaconda-client\n",
    "+ conda update anaconda\n",
    "+ pip install azureml-sdk[notebooks,automl]\n",
    "+ pip install azureml-dataprep[pandas]\n",
    "+ conda update conda\n",
    "\n",
    "**Note: If you need to upgrade the azureml components, uninstall the old version first before install the new ones.**\n",
    "\n",
    "OR you can use a [free Microsoft Azure Notebooks](https://notebooks.azure.com/) to run this notebook if you don't have Anaconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Azure Machine Learning SDK for Python \n",
    "\n",
    "This step is to check that you have installed Azure Machine Learning SDK for Python.\n",
    "\n",
    "**Note: if you encounter ModuleNotFoundError, try uninstall all the azureml components first then re-install them again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "check version"
    ]
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "# check core SDK version number (need Python 3.6 kernel if you run this in Microsoft Azure Notebooks)\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Azure Machine Learning Workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `workspace`.\n",
    "\n",
    "If you see this message:\n",
    "\"Performing interactive authentication. Please follow the instructions on the terminal.\n",
    "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code &lt;token\\&gt; to authenticate.\"\n",
    "    \n",
    "Click on the link and use the &lt;token\\&gt; given to authenticate. After authenticated, run this script again to get load the Workspace.&lt;/token\\&gt;&lt;/token\\&gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "load workspace"
    ]
   },
   "outputs": [],
   "source": [
    "# Load workspace configuration from the config.json file in the current folder.\n",
    "from azureml.core import Workspace\n",
    "workspace = Workspace.from_config()\n",
    "print(workspace.name, workspace.location, workspace.resource_group, workspace.location, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Experiment\n",
    "\n",
    "An Experiment tracks the runs in your workspace. A workspace can have muliple experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create experiment"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'predict-employee-retention'\n",
    "exp = Experiment(workspace=workspace, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Datastore\n",
    "\n",
    "From your machine learning Workspace, launch Azure Machine Learning studio. Click \"Datastores\" (under \"Manage\" section) on the left menu.\n",
    "\n",
    "This tutorial will use the default datastore 'workspaceblobstore' to store the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to datastore\n",
    "from azureml.core import Datastore\n",
    "datastore = Datastore.get(workspace, datastore_name='workspaceblobstore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "\n",
    "To upload your data file, launch Azure Machine Learning studio then go to \"Datasets\" (under \"Assets\" section), then click \"Create dataset\".\n",
    "\n",
    "You use training-data.csv to create the following datasets (note the date time in the \"Upload path\" is auto generated):\n",
    "1. Name=predict-employee-retention-tabular, Dataset type=Tabular, Upload path=predict-employee-retention-tabular/04-17-2020_033204_UTC/training-data.csv, select \"Use headers from the first file\"\n",
    "\n",
    "When you upload training-data.csv as a Tabular dataset in the datastore, make sure to uncheck \"Skip data validation\" so that it can auto detect the data type.\n",
    "\n",
    "2. Name=predict-employee-retention-training-data, Dataset type=File, Upload path=predict-employee-retention/04-16-2020_023057_UTC/training-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to download data from datastore\n",
    "\n",
    "This section shows how to download a dataset (file or tabular).\n",
    "\n",
    "[Create Azure Machine Learning datasets](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "import os\n",
    "\n",
    "# 1. Download data file from datastore using \"Relative path\"\n",
    "# Notice the downloaded file will be stored in a folder that has structure similar to the relative path.\n",
    "temp_folder = os.path.join(os.getcwd(), \"temp\")\n",
    "# Remember to change file relative path.\n",
    "training_data_file = 'predict-employee-retention/05-21-2020_072626_UTC/training-data.csv'\n",
    "os.makedirs(temp_folder, exist_ok=True)\n",
    "datastore.download(target_path=temp_folder,\n",
    "                   prefix=training_data_file,\n",
    "                   show_progress=True,\n",
    "                   overwrite=True)\n",
    "\n",
    "# 2. Download Datasets from Workspace\n",
    "# Below is generated sample code found in \"Microsoft Azure Machine Learning > Datasets > (choose your dataset) > Consume\". You can modify it to suit your need.\n",
    "# azureml-core of version 1.0.72 or higher is required\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "#subscription_id = 'XXX'\n",
    "#resource_group = 'XXX'\n",
    "#workspace_name = 'XXX'\n",
    "#workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name='predict-employee-retention-training-data')\n",
    "dataset.download(target_path=temp_folder, overwrite=True)\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name='predict-employee-retention-tabular')\n",
    "df = dataset.to_pandas_dataframe()\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "\n",
    "This example uses Dataset.get_by_name() to retrieve the registered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azureml.core import Dataset\n",
    "import os\n",
    "\n",
    "# Read tabular data that has been uploaded into datastore.\n",
    "training_data = Dataset.get_by_name(workspace, name='predict-employee-retention-tabular')\n",
    "dataset = training_data.to_pandas_dataframe()\n",
    "\n",
    "# Rename sales feature into department\n",
    "dataset = dataset.rename(columns={\"sales\": \"department\"})\n",
    "\n",
    "# Map salary into integers\n",
    "salary_map = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "dataset[\"salary\"] = dataset[\"salary\"].map(salary_map)\n",
    "display(dataset.head())\n",
    "\n",
    "# Create dummy variables for department feature\n",
    "dataset = pd.get_dummies(dataset, columns=[\"department\"], drop_first=True)\n",
    "# Now becomes 17 features after spliting up department\n",
    "#dataset.columns[dataset.columns != \"left\"].shape\n",
    "display(dataset.head())\n",
    "\n",
    "# Check both the datatypes and if there is missing values\n",
    "print(\"\\033[1m\" + \"\\033[94m\" + \"Data types:\\n\" + 11 * \"-\")\n",
    "print(\"\\033[30m\" + \"{}\\n\".format(dataset.dtypes))\n",
    "print(\"\\033[1m\" + \"\\033[94m\" + \"Sum of null values in each column:\\n\" + 35 * \"-\")\n",
    "print(\"\\033[30m\" + \"{}\".format(dataset.isnull().sum()))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Get number of positive and negative examples\n",
    "pos = dataset[dataset[\"left\"] == 1].shape[0]\n",
    "neg = dataset[dataset[\"left\"] == 0].shape[0]\n",
    "print(\"Positive examples = {}\".format(pos))\n",
    "print(\"Negative examples = {}\".format(neg))\n",
    "print(\"Proportion of positive to negative examples = {:.2f}%\".format((pos / neg) * 100))\n",
    "sns.countplot(dataset[\"left\"])\n",
    "plt.xticks((0, 1), [\"Didn't leave\", \"Left\"])\n",
    "plt.xlabel(\"Left\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model using SKLearn\n",
    "\n",
    "First, split the data into training and test sets using 80/20 split; 80% of the data will be used to train the models and 20% to test the performance of the models. Second, Upsample the minority class and downsample the majority class. \n",
    "\n",
    "For this data set, positive class refers to those that \"Left\", negative class refers to those that \"Didn't leave\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Convert dataframe into numpy objects and split them into\n",
    "# train and test sets: 80/20\n",
    "X = dataset.loc[:, dataset.columns != \"left\"].values\n",
    "y = dataset.loc[:, dataset.columns == \"left\"].values.flatten()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "\n",
    "# Upsample minority class (When \"Left\" is minority, \"Didn't leave\" is majority)\n",
    "X_train_u, y_train_u = resample(X_train[y_train == 1],\n",
    "                                y_train[y_train == 1],\n",
    "                                replace=True,\n",
    "                                n_samples=X_train[y_train == 0].shape[0],\n",
    "                                random_state=1)\n",
    "X_train_u = np.concatenate((X_train[y_train == 0], X_train_u))\n",
    "y_train_u = np.concatenate((y_train[y_train == 0], y_train_u))\n",
    "\n",
    "# Downsample majority class (When \"Left\" is majority, \"Didn't leave\" is minority)\n",
    "X_train_d, y_train_d = resample(X_train[y_train == 0],\n",
    "                                y_train[y_train == 0],\n",
    "                                replace=True,\n",
    "                                n_samples=X_train[y_train == 1].shape[0],\n",
    "                                random_state=1)\n",
    "X_train_d = np.concatenate((X_train[y_train == 1], X_train_d))\n",
    "y_train_d = np.concatenate((y_train[y_train == 1], y_train_d))\n",
    "\n",
    "print(\"Original shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Upsampled shape:\", X_train_u.shape, y_train_u.shape)\n",
    "print(\"Downsampled shape:\", X_train_d.shape, y_train_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "clf=LogisticRegression(solver='liblinear',random_state=0)\n",
    "\n",
    "# Choose training data\n",
    "print('Use original data')\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#print('Use upsampled data')\n",
    "#clf.fit(X_train_u,y_train_u)\n",
    "\n",
    "#print('Use downsampled sample')\n",
    "#clf.fit(X_train_d,y_train_d)\n",
    "\n",
    "# View the model's coefficients and bias\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "\n",
    "y_pred_LR=clf.predict(X_test)\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred_LR)\n",
    "\n",
    "# Display confusion matrix\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "# Show stats of binary classification.\n",
    "# Accuracy is sum of diagonal divided by total observations.\n",
    "accuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))\n",
    "\n",
    "# Alternative way to calculate accuracy on the prediction\n",
    "#acc = np.average(y_pred_LR == y_test)\n",
    "#print('Accuracy is', acc)\n",
    "\n",
    "#Metrics for Binary Confusion Matrices\n",
    "precision = cf_matrix[1,1] / sum(cf_matrix[:,1])\n",
    "recall    = cf_matrix[1,1] / sum(cf_matrix[1,:])\n",
    "f1_score  = 2*precision*recall / (precision + recall)\n",
    "stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision (ability to identify only the relevant data points)={:0.3f}[ True Pos/(True Pos+False Pos)]\\nRecall (ability to find all the data points of interest in a dataset)={:0.3f} [True Pos/(True Pos+False Neg)]\\nF1 Score (harmonic mean of precision and recall taking both metrics into account) ={:0.3f} [2 x (precision*recall/precision+recall)]\".format(\n",
    "accuracy,precision,recall,f1_score)\n",
    "print(stats_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "For this task, submit a job to train model in your local machine. To submit a job you:\n",
    "* Create directory to store training scripts and data\n",
    "* Create training scripts\n",
    "* Create training environment\n",
    "* Submit a run\n",
    "\n",
    "The training results will be stored in your Azure ML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Create directory to store model training scripts and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"training\")\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "data_folder = os.path.join(script_folder, \"data\")\n",
    "os.makedirs(data_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Create training scripts\n",
    "\n",
    "Create training scripts in the directory you just created. Notice how the script saves the model:\n",
    "    \n",
    "+ The training script saves your model into a directory named outputs. <br>\n",
    "`joblib.dump(value=clf, filename='outputs/predict-employee-retention-model.pkl')`<br>\n",
    "Anything written in this directory is automatically uploaded into your workspace. You'll access your model from this directory later in the tutorial.\n",
    "\n",
    "[Run class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py)\n",
    "\n",
    "[Joblib](https://joblib.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_csv.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "from predict_employee_retention import generate_model\n",
    "from joblib import dump\n",
    "\n",
    "# retrieve argument configured through script_params in estimator\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset-file', type=str, dest='dataset_file', help='path and name of the dataset file')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "training_dataset_file = args.dataset_file\n",
    "run.log('Dataset name', training_dataset_file)\n",
    "\n",
    "# Read dataset\n",
    "dataset = pd.read_csv(training_dataset_file)\n",
    "run.log('Read training data from file', training_dataset_file)\n",
    "\n",
    "# Generate model\n",
    "clf = generate_model(dataset, run)\n",
    "\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "dump(value=clf, filename='outputs/predict-employee-retention-model.pkl')\n",
    "run.log('End of run','Training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predict_employee_retention.py\n",
    "\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def generate_model(dataset, run):\n",
    "    # Rename sales feature into department\n",
    "    dataset = dataset.rename(columns={\"sales\": \"department\"})\n",
    "\n",
    "    # Map salary into integers\n",
    "    salary_map = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "    dataset[\"salary\"] = dataset[\"salary\"].map(salary_map)\n",
    "\n",
    "    # Create dummy variables for department feature\n",
    "    dataset = pd.get_dummies(dataset, columns=[\"department\"], drop_first=True)\n",
    "\n",
    "    # Get number of positve and negative examples\n",
    "    pos = dataset[dataset[\"left\"] == 1].shape[0]\n",
    "    neg = dataset[dataset[\"left\"] == 0].shape[0]\n",
    "    run.log('Positive','Positive examples = {}'.format(pos))\n",
    "    run.log('Negative', 'Negative examples = {}'.format(neg))\n",
    "    run.log('Proportion','Proportion of positive to negative examples = {:.2f}%'.format((pos / neg) * 100))\n",
    "\n",
    "    run.log('Begin training','Train with a logistic regression model.')\n",
    "    # Convert dataframe into numpy objects and split them into\n",
    "    # train and test sets: 80/20\n",
    "    X = dataset.loc[:, dataset.columns != \"left\"].values\n",
    "    y = dataset.loc[:, dataset.columns == \"left\"].values.flatten()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "\n",
    "    clf=LogisticRegression(solver='liblinear',random_state=0)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    # View the model's coefficients and bias\n",
    "    run.log('Coefficients',clf.coef_)\n",
    "    run.log('Bias',clf.intercept_)\n",
    "\n",
    "    run.log('Predict','Predict the training data set.')\n",
    "    y_pred_LR=clf.predict(X_test)\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred_LR)\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [f\"{v1} {v2} ({v3})\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    run.log('Confusion Matrix',labels)\n",
    "\n",
    "    # Display statistics\n",
    "    accuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))\n",
    "    run.log('Accuracy',accuracy)\n",
    "    precision = cf_matrix[1,1] / sum(cf_matrix[:,1])\n",
    "    run.log('Precision',precision)\n",
    "    recall    = cf_matrix[1,1] / sum(cf_matrix[1,:])\n",
    "    run.log('Recall',recall)\n",
    "    f1_score  = 2*precision*recall / (precision + recall)\n",
    "    run.log('F1-score',f1_score)\n",
    "    stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "    accuracy,precision,recall,f1_score)\n",
    "    run.log('Show stats',stats_text)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy scripts to the folder used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('train_csv.py', script_folder)\n",
    "shutil.copy('predict_employee_retention.py', script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Option 1: Download data file (.csv) from datastore\n",
    "\n",
    "In this example, you download a FileDataset (i.e. training-data.csv) from the datastore and use it as an input to your training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Refer to \"Explore ways to download data from datastore\" for details.\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name='predict-employee-retention-training-data')\n",
    "dataset.download(target_path=data_folder, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Create training environment in local machine\n",
    "\n",
    "The steps here is to create a local training environment, such as to leverage on the Anaconda installed on local machine. However, you can also run this \"locally\" in the Microsoft Azure Notebooks.\n",
    "\n",
    "Details are provided at https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#local\n",
    "    \n",
    "[What are Azure Machine Learning environments?](https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a user-managed environment\n",
    "By default, Azure Machine Learning service will build a Conda environment with dependencies you specified, and will execute the run in that environment instead of using any Python libraries that you installed on the base image.\n",
    "A later example in this example will demonstrate the use of Environment when training the model on Azure. \n",
    "\n",
    "In some situations, your custom base image may already contain a Python environment with packages that you want to use.\n",
    "\n",
    "When using a user-managed environment for local training, you are responsible for ensuring that all the necessary packages are available in the Python environment you choose to run the script in.\n",
    "\n",
    "+ Create and attach: There's no need to create or attach a compute target to use your local computer as the training environment.\n",
    "+ Configure: When you use your local computer as a compute target, the training code is run in your development environment. If that environment already has the Python packages you need, use the user-managed environment.\n",
    "\n",
    "To use your own installed packages, set the parameter Environment.python.user_managed_dependencies = True. Ensure that the base image contains a Python interpreter, and has the packages your training script needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# Create a 'user-managed environment' environment.\n",
    "user_managed_env = Environment(\"user-managed-env\")\n",
    "\n",
    "user_managed_env.python.user_managed_dependencies = True\n",
    "\n",
    "# You can choose a specific Python environment by pointing to a Python path \n",
    "#user_managed_env.python.interpreter_path = '/home/johndoe/miniconda3/envs/myenv/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ScriptRunConfig\n",
    "\n",
    "Whatever the way you manage your environment, you need to use the ScriptRunConfig class. ScriptRunConfig identifies the training script to run in the experiment and the environment in which to run it. \n",
    "\n",
    "ScriptRunConfig includes\n",
    "+ source_directory: The source directory that contains your training script\n",
    "+ script: Identify the training script\n",
    "+ run_config: The run configuration, which in turn defines where the training will occur\n",
    "\n",
    "Note: ScriptRunConfig doesn't allow you to pass dataset to the training script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder, script='train_csv.py', arguments=['--dataset-file','./data/training-data.csv'])\n",
    "src.run_config.environment = user_managed_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altenatively, you can use a local target. You can switch the same experiment to run in a different compute target by using a different run configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Edit a run configuration property on the fly.\n",
    "run_local = RunConfiguration()\n",
    "run_local.environment.python.user_managed_dependencies = True\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder, script='train_csv.py', run_config=run_local, arguments=['--dataset-file','./data/training-data.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Submit a run\n",
    "\n",
    "After you create a run configuration, you use it to run your experiment. An experiment is a logical container in an Azure ML Workspace. It contains a series of trials called Runs. As such, it hosts run records such as run metrics, logs, and other output artifacts from your experiments.\n",
    "\n",
    "The code pattern to submit a training run is the same for all types of compute targets:\n",
    "+ Create an experiment to run.\n",
    "+ Submit the run.\n",
    "+ Wait for the run to complete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(src)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the run to complete\n",
    "\n",
    "After you submit the run, you can immediately execute this code to watch the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10 to 15 seconds until the job finishes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get log results upon completion\n",
    "\n",
    "Model training and monitoring happen in the background. Wait until the model has finished training before you run more code. Use wait_for_completion to show when the model training is finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=False)  # specify True for a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All these calculations were run on your local machine, in the conda environment you defined above. You can find the results in:\n",
    "\n",
    "    + ~/.azureml/envs/azureml_xxxx for the conda environment you just created\n",
    "    + ~/AppData/Local/Temp/azureml_runs/train-on-local_xxxx for the machine learning models you trained (this path may differ depending on the platform you use). This folder also contains\n",
    "        - Logs (under azureml_logs/)\n",
    "        - Output pickled files (under outputs/)\n",
    "        - The configuration files (credentials, local and docker image setups)\n",
    "        - The train.py and mylib.py scripts\n",
    "        - The current notebook\n",
    "\n",
    "Take a few minutes to examine the output of the cell above. It shows the content of some of the log files, and extra information on the conda environment used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display run results\n",
    "\n",
    "Display the information captured by run.log(). Results will appear only after the run completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "\n",
    "The last step in the training script wrote the file outputs/predict-employee-retention-model.pkl in a directory named outputs in the VM of the cluster where the job is run. \"outputs\" is a special directory in that all content in this directory is automatically uploaded to your workspace. This content appears in the run record in the experiment under your workspace. So the model file is now also available in your workspace.\n",
    "\n",
    "You can see files associated with that run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model in the workspace, so that you or other collaborators can later query, examine, and deploy this model. You can store the metrics you captured and store them into \"tags\" in the Model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to tags so that these information can be used for model comparison purpose.\n",
    "metrics = ['Accuracy','Precision','Recall','F1-score']\n",
    "tags = {}\n",
    "for key in metrics:\n",
    "    tags[key] = run.get_metrics(key).get(key)\n",
    "\n",
    "# register model, note the metric values are stored in \"tags\".\n",
    "model = run.register_model(model_name='predict-employee-retention-model',\n",
    "                           model_path='outputs/predict-employee-retention-model.pkl',\n",
    "                           tags=tags\n",
    "                          )\n",
    "print(model.name, model.id, model.version, model.tags, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have registered the model, you can proceed to Tutorial#2 to deploy the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Use input dataset to pass TabularDataset to training scripts\n",
    "\n",
    "In this example, you download a TabularDataset from datastore and use it as a direct input to your estimator object for training in your local machine.\n",
    "\n",
    "TabularDataset objects provide the ability to load the data into a pandas or spark DataFrame so that you can work with familiar data preparation and training libraries. To leverage this capability, you can pass a TabularDataset as the input in your training configuration, and then retrieve it in your script.\n",
    "\n",
    "To do so, access the input dataset through the Run object in your training script and use the to_pandas_dataframe() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_TabularDataset.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "from predict_employee_retention import generate_model\n",
    "\n",
    "import joblib\n",
    "\n",
    "from azureml.core import Run, Dataset\n",
    "\n",
    "# retrieve argument configured through script_params in estimator\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset-name', type=str, dest='dataset_name', help='Name of dataset')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "training_dataset_name = args.dataset_name\n",
    "run.log('Dataset name', training_dataset_name)\n",
    "\n",
    "# get the input dataset by name\n",
    "training_data = run.input_datasets[training_dataset_name]\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "dataset = training_data.to_pandas_dataframe()\n",
    "\n",
    "# Generate model\n",
    "clf = generate_model(dataset, run)\n",
    "\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=clf, filename='outputs/predict-employee-retention-model.pkl')\n",
    "run.log('End of run','Training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('train_TabularDataset.py', script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download training data from data source\n",
    "\n",
    "This step shows how to download a TabularDataset from datasource. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "tabular_dataset = Dataset.get_by_name(workspace, name='predict-employee-retention-tabular')\n",
    "display(tabular_dataset.to_pandas_dataframe().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an Estimator object\n",
    "An Estimator object is used to submit the experiment run. Azure Machine Learning has pre-configured estimators for common machine learning frameworks, as well as a generic estimator.\n",
    "\n",
    "This code creates a SKLearn estimator object, est, that specifies\n",
    "+ A script directory for your scripts. All the files in this directory are uploaded into the cluster nodes for execution.\n",
    "+ Parameters required from the training script \n",
    "+ The environment definition for the experiment.\n",
    "+ The training script i.e. train_TabularDataset.py.\n",
    "+ The compute target for the experiment. In this case use 'local'.\n",
    "+ The input dataset for training, 'training_data' as_named_input() is required so that the input dataset can be referenced by the assigned name in your training script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a user-managed environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# Create a 'user-managed environment' environment.\n",
    "user_managed_env = Environment(\"user-managed-env\")\n",
    "\n",
    "user_managed_env.python.user_managed_dependencies = True\n",
    "\n",
    "# You can choose a specific Python environment by pointing to a Python path \n",
    "#user_managed_env.python.interpreter_path = '/home/johndoe/miniconda3/envs/myenv/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SKLearn estimator object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn\n",
    "\n",
    "script_params = {\n",
    "    '--dataset-name': 'training_data'\n",
    "}\n",
    "\n",
    "est = SKLearn(source_directory=script_folder,\n",
    "              script_params=script_params,\n",
    "              environment_definition=user_managed_env,\n",
    "              entry_script='train_TabularDataset.py',\n",
    "              compute_target='local',\n",
    "              # pass dataset object as an input with name 'training_data'\n",
    "              inputs=[tabular_dataset.as_named_input('training_data')]\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submit the estimator as part of your experiment run\n",
    "run = exp.submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the run to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get log results upon completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding metrics to tags so that these information can be used for model comparison purpose.\n",
    "metrics = ['Accuracy','Precision','Recall','F1-score']\n",
    "tags = {}\n",
    "for key in metrics:\n",
    "    tags[key] = run.get_metrics(key).get(key)\n",
    "\n",
    "# register model, note the metric values are stored in \"tags\".\n",
    "model = run.register_model(model_name='predict-employee-retention-model',\n",
    "                           model_path='outputs/predict-employee-retention-model.pkl',\n",
    "                           tags=tags\n",
    "                          )\n",
    "print(model.name, model.id, model.version, model.tags, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have registered the model, you can proceed to Tutorial#2 to deploy the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following examples are for training the model on Azure\n",
    "\n",
    "This example is similar to the TabularDataset example above, the change is to train the model using cloud compute resource, instead of local machine.\n",
    "\n",
    "Note: The joblib need to change to use older version of SKLearn in the training script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support.\n",
    "\n",
    "In this tutorial, you create an Azure Machine Learning compute cluster as your training environment. Once it is created, you can find 'cpucluster' in your Workspace under **Compute &gt; Training clusters**.\n",
    "\n",
    "Note:\n",
    "\n",
    "To avoid charges when no jobs are running, set the minimum nodes to 0. This setting allows Azure Machine Learning to de-allocate the nodes when they aren't in use. Any value larger than 0 will keep that number of nodes running, even if they are not in use.\n",
    "\n",
    "[Set up and use compute targets for model training](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create mlc",
     "amlcompute"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 2)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_DS2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in workspace.compute_targets:\n",
    "    compute_target = workspace.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes,\n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(\n",
    "        workspace, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_TabularDataset2.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "from predict_employee_retention import generate_model\n",
    "\n",
    "from sklearn.externals import joblib # Use this when test model created on cloud (code error if use joblib from latest sklearn)\n",
    "\n",
    "from azureml.core import Run, Dataset\n",
    "\n",
    "# retrieve argument configured through script_params in estimator\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset-name', type=str, dest='dataset_name', help='Name of dataset')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "training_dataset_name = args.dataset_name\n",
    "run.log('Dataset name', training_dataset_name)\n",
    "\n",
    "# get the input dataset by name\n",
    "training_data = run.input_datasets[training_dataset_name]\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "dataset = training_data.to_pandas_dataframe()\n",
    "\n",
    "# Generate model\n",
    "clf = generate_model(dataset, run)\n",
    "\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=clf, filename='outputs/predict-employee-retention-model.pkl')\n",
    "run.log('End of run','Training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('train_TabularDataset2.py', script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment object that specifies the dependencies required for training\n",
    "\n",
    "Note: need to use older version of scikit-learn due to incompatible joblib package issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "env = Environment('my_env')\n",
    "\n",
    "# Need to use an older version of scklearn due to issue with Joblib when saving pickle file.\n",
    "# Will fail to run https://predictemployeeretention-xxx.notebooks.azure.com/j/notebooks/predict-employee-retention-part1-training.ipynb#Submit-a-run-to-use-the-cloud-compute if use joblib from latest sklearn.\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk','scikit-learn==0.20.3','azureml-dataprep[pandas,fuse]>=1.1.14'])\n",
    "\n",
    "env.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a run to use the cloud compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn\n",
    "\n",
    "script_params = {\n",
    "    '--dataset-name': 'training_data'\n",
    "}\n",
    "\n",
    "est = SKLearn(source_directory=script_folder,\n",
    "              script_params=script_params,\n",
    "              environment_definition=env,\n",
    "              entry_script='train_TabularDataset2.py',\n",
    "              compute_target=compute_target,\n",
    "              # pass dataset object as an input with name 'training_data'\n",
    "              inputs=[tabular_dataset.as_named_input('training_data')]\n",
    "              )\n",
    "\n",
    "# Submit the estimator as part of your experiment run\n",
    "run = exp.submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the run to complete\n",
    "\n",
    "Will take a longer time to run on cloud because it will pip for the specified dependencies to be installed into the environment. Launch the widget to watch the progress. You can re-run this code to relaunch the widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage environment\n",
    "\n",
    "Manage environments so that you can update, track, and reuse them across compute targets and with other users of the workspace.\n",
    "\n",
    "### Register environment\n",
    "\n",
    "The environment is automatically registered with your workspace when you submit a run or deploy a web service. You can also manually register the environment by using the register() method. This operation makes the environment into an entity that's tracked and versioned in the cloud. The entity can be shared between workspace users.\n",
    "\n",
    "The following code registers the \"my_env\" environment to the workspace. Please remember to register this environment, which will be reused later.\n",
    "\n",
    "[Reuse environments for training and deployment by using Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.register(workspace=workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "Wait for the run to complete before you register the model. Otherwise you will see ModelPathNotFoundException."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metrics to tags so that these information can be used for model comparison purpose.\n",
    "metrics = ['Accuracy','Precision','Recall','F1-score']\n",
    "tags = {}\n",
    "for key in metrics:\n",
    "    tags[key] = run.get_metrics(key).get(key)\n",
    "\n",
    "# register model, note the metric values are stored in \"tags\".\n",
    "model = run.register_model(model_name='predict-employee-retention-model',\n",
    "                           model_path='outputs/predict-employee-retention-model.pkl',\n",
    "                           tags=tags\n",
    "                          )\n",
    "print(model.name, model.id, model.version, model.tags, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have registered the model, you can proceed to Tutorial#2 to deploy the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3:  Mount data file to a remote compute target for training script to access\n",
    "\n",
    "When you mount a dataset, you attach the files referenced by the dataset to a directory (mount point) and make it available on the compute target.\n",
    "\n",
    "FileDataset represents a collection of file references in datastores or public URLs to use in Azure Machine Learning.\n",
    "\n",
    "The actual data loading happens when FileDataset is asked to deliver the data into another storage mechanism (e.g. files downloaded or mounted to local path).\n",
    "\n",
    "[FileDataset class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py)\n",
    "\n",
    "[Create Azure Machine Learning datasets](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data file from datastore\n",
    "\n",
    "You download a FileDataset (i.e. training-data.csv) from the datastore, then mount it.\n",
    "\n",
    "There are 2 ways to retrieve FileDataset from the datastore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "# Method 1: this example use \"Dataset.File.from_files\" method.\n",
    "# For this method to work on mounting a single file, the FileDataset must specify relative path including file name.\n",
    "# Remember to change the file relative path according to the path! You can check the path from the Datasets in Azure Machine Learning Studio\n",
    "\n",
    "file_dataset = Dataset.File.from_files(path = [(datastore, 'predict-employee-retention/05-21-2020_072626_UTC/training-data.csv')])\n",
    "\n",
    "# Although the following code can retrieve the FileDataset, but the mount don't work:\n",
    "# file_dataset = Dataset.File.from_files(path = [(datastore, 'predict-employee-retention')])\n",
    "# You will see this error:\n",
    "# Error occurred: User program failed with ParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
    "\n",
    "print(file_dataset)\n",
    "\n",
    "# To see the list of files referenced by the dataset, if any others.\n",
    "file_dataset.to_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "# Method 2: this example use \"Dataset.get_by_name\" method.\n",
    "file_dataset = Dataset.get_by_name(workspace, name='predict-employee-retention-training-data')\n",
    "print(file_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_FileDataset.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "from predict_employee_retention import generate_model\n",
    "\n",
    "from sklearn.externals import joblib # Use this when test model created on cloud (code error if use joblib from latest sklearn)\n",
    "\n",
    "# retrieve argument configured through script_params in estimator\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset-file', type=str, dest='dataset_file', help='path and name of the dataset file')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "training_dataset_file = args.dataset_file\n",
    "\n",
    "run.log('Dataset name', training_dataset_file)\n",
    "\n",
    "# Read dataset\n",
    "dataset = pd.read_csv(training_dataset_file)\n",
    "run.log('Read training data from file', training_dataset_file)\n",
    "\n",
    "# Generate model\n",
    "clf = generate_model(dataset, run)\n",
    "\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=clf, filename='outputs/predict-employee-retention-model.pkl')\n",
    "run.log('End of run','Training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('train_FileDataset.py', script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-use an existing environment for training\n",
    "\n",
    "This example here will reuse an existing environment that has been saved to the workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View a list of environments\n",
    "\n",
    "View the environments in your workspace by using the Environment.list(workspace=\"workspace_name\") class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "\n",
    "## Get the list of environments\n",
    "env_list = Environment.list(workspace)\n",
    "# Get 'my_env' that was saved earlier.\n",
    "env_list.get('my_env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get a specific environment by name and version. The following code uses the get() method to retrieve version 1 of my_env environment from the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "\n",
    "# Retrieve the latest version of the Environment\n",
    "env = Environment.get(workspace=workspace,name=\"my_env\")\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SKLearn estimator object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn\n",
    "\n",
    "script_params = {\n",
    "    # mount the dataset on the remote compute and pass the mounted path as an argument to the training script\n",
    "    '--dataset-file': file_dataset.as_named_input('dataset_file').as_mount('/tmp/training_data'),\n",
    "}\n",
    "\n",
    "est = SKLearn(source_directory=script_folder,\n",
    "              script_params=script_params,\n",
    "              environment_definition=env,\n",
    "              entry_script='train_FileDataset.py',\n",
    "              compute_target=compute_target\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a run to use the cloud compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submit the estimator as part of your experiment run\n",
    "run = exp.submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the run to complete\n",
    "\n",
    "Launch the widget to watch the progress. You can re-run this code to relaunch the widget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "Wait for the run to complete before you register the model. Otherwise you will see ModelPathNotFoundException."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Adding metrics to tags so that these information can be used for model comparison purpose.\n",
    "metrics = ['Accuracy','Precision','Recall','F1-score']\n",
    "tags = {}\n",
    "for key in metrics:\n",
    "    tags[key] = run.get_metrics(key).get(key)\n",
    "\n",
    "# register model, note the metric values are stored in \"tags\".\n",
    "model = run.register_model(model_name='predict-employee-retention-model',\n",
    "                           model_path='outputs/predict-employee-retention-model.pkl',\n",
    "                           tags=tags\n",
    "                          )\n",
    "print(model.name, model.id, model.version, model.tags, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have registered the model, you can proceed to Tutorial#2 to deploy the model."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "msauthor": "roastala"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
